import streamlit as st
from openai import OpenAI
from datetime import datetime

# =============================
# 1Ô∏è‚É£ C·∫•u h√¨nh & UI Setup
# (Gi·ªØ nguy√™n ph·∫ßn n√†y)
# =============================
st.set_page_config(page_title="LLM Tool - Ollama (Local)", layout="centered")
st.markdown(
    """
    <style>
    .block-container { max-width: 1000px !important; padding-top: 2rem; }
    textarea { font-family: Consolas, monospace !important; background-color: #f8f9fa; border-radius: 8px; }
    h1, h2, h3 { color: #28a745; border-bottom: 2px solid #e0e0e0; padding-bottom: 10px; margin-top: 20px; }
    .stButton>button { width: 100%; height: 50px; font-weight: bold; background-color: #28a745; color: white; border: none; border-radius: 10px; transition: background-color 0.3s ease, transform 0.1s; box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1); }
    .stButton>button:hover { background-color: #1e7e34; transform: translateY(-2px); }
    div[role="radiogroup"] { padding: 10px; border: 1px solid #ced4da; border-radius: 8px; background-color: #f8f9fa; margin-bottom: 20px; }
    .streamlit-expanderHeader { background-color: #e9ecef; border-radius: 8px; padding: 10px; margin-top: 5px; font-weight: 500; }
    </style>
    """,
    unsafe_allow_html=True
)

st.title("üê≥ LLM Tool - Ollama (Local)")
st.write("·ª®ng d·ª•ng s·ª≠ d·ª•ng **Ollama API c·ª•c b·ªô**")

# =============================
# 2Ô∏è‚É£ Ollama Client (Thay th·∫ø OpenAI)
# =============================
client = OpenAI(
    base_url="http://localhost:11434/v1",
    api_key="ollama_key_placeholder"
)

# M√¥ h√¨nh c·ª•c b·ªô
OLLAMA_MODEL = "gemma3:270m"
# =============================
# 3Ô∏è‚É£ L∆∞u l·ªãch s·ª≠
# =============================
if "history" not in st.session_state:
    st.session_state.history = []

# =============================
# 4Ô∏è‚É£ Giao di·ªán ng∆∞·ªùi d√πng
# =============================
st.subheader("‚öôÔ∏è C·∫•u h√¨nh m√¥ h√¨nh")
model_names = [OLLAMA_MODEL]
default_index = 0
model = st.selectbox("M√¥ h√¨nh ƒëang ch·∫°y tr√™n Ollama", model_names, index=default_index, disabled=True)

st.warning(
    f"**L∆∞u √Ω:** ·ª®ng d·ª•ng ƒëang ch·∫°y m√¥ h√¨nh **{OLLAMA_MODEL}** c·ª•c b·ªô. Vui l√≤ng ƒë·∫£m b·∫£o Ollama ƒëang ch·∫°y v√† ki√™n nh·∫´n ch·ªù ƒë·ª£i do h·∫°n ch·∫ø v·ªÅ t√†i nguy√™n.")

col1, col2 = st.columns([3, 1])
with col2:
    st.text("")
    st.text("")
    clear = st.button("üóëÔ∏è X√≥a l·ªãch s·ª≠")
if clear:
    st.session_state.history = []
    st.rerun()

st.subheader("üìù Nh·∫≠p d·ªØ li·ªáu")

# --- ƒêI·ªÄU CH·ªàNH TASK_PROMPTS T·∫†I ƒê√ÇY ---
TASK_PROMPTS = {
    # Th√™m r√†ng bu·ªôc "ch·ªâ tr·∫£ v·ªÅ..." v√† vai tr√≤ chuy√™n gia
    "T√≥m t·∫Øt": "B·∫°n l√† chuy√™n gia t√≥m t·∫Øt. T√≥m t·∫Øt n·ªôi dung sau b·∫±ng ti·∫øng Vi·ªát, t·∫≠p trung v√†o c√°c √Ω ch√≠nh v√† r√∫t g·ªçn th√†nh 1-2 c√¢u. Ch·ªâ tr·∫£ v·ªÅ n·ªôi dung t√≥m t·∫Øt:",

    # Y√™u c·∫ßu ng√¥n ng·ªØ c·ª• th·ªÉ v√† kh√¥ng gi·∫£i th√≠ch
    "D·ªãch sang ti·∫øng Ph√°p": "B·∫°n l√† chuy√™n gia d·ªãch thu·∫≠t ti·∫øng Ph√°p. D·ªãch c√¢u sau sang ti·∫øng Ph√°p chu·∫©n, ch·ªâ tr·∫£ v·ªÅ n·ªôi dung d·ªãch (kh√¥ng k√®m theo b·∫•t k·ª≥ gi·∫£i th√≠ch, ch√†o h·ªèi, hay ti√™u ƒë·ªÅ n√†o kh√°c):",

    # Gi·ªØ vai tr√≤ th√¢n thi·ªán, y√™u c·∫ßu d√πng t·ª´ ng·ªØ ƒë∆°n gi·∫£n
    "Gi·∫£i th√≠ch ƒë∆°n gi·∫£n": "B·∫°n l√† m·ªôt gi√°o vi√™n th√¢n thi·ªán. Gi·∫£i th√≠ch n·ªôi dung sau b·∫±ng ng√¥n ng·ªØ c·ª±c k·ª≥ ƒë∆°n gi·∫£n, d·ªÖ hi·ªÉu, ch·ªâ d√πng t·ª´ ng·ªØ d√†nh cho h·ªçc sinh l·ªõp 5:",

    # R√†ng bu·ªôc format output c·ª• th·ªÉ
    "Tr√≠ch xu·∫•t t·ª´ kh√≥a": "Tr√≠ch xu·∫•t 5 t·ª´ kh√≥a ho·∫∑c c·ª•m t·ª´ quan tr·ªçng nh·∫•t t·ª´ vƒÉn b·∫£n sau. M·ªói t·ª´ kh√≥a ph·∫£i n·∫±m tr√™n m·ªôt d√≤ng ri√™ng bi·ªát, kh√¥ng ƒë√°nh s·ªë th·ª© t·ª± v√† kh√¥ng c√≥ k√Ω t·ª± ƒë·∫∑c bi·ªát n√†o kh√°c:",

    # Y√™u c·∫ßu c√∫ ph√°p code v√† gi·∫£i th√≠ch ng·∫Øn g·ªçn
    "T·∫°o m√£ Python": "B·∫°n l√† m·ªôt l·∫≠p tr√¨nh vi√™n Python chuy√™n nghi·ªáp. Vi·∫øt m√£ Python ƒë·ªÉ th·ª±c hi·ªán y√™u c·∫ßu sau, k√®m theo gi·∫£i th√≠ch ng·∫Øn g·ªçn. ƒê·∫∑t m√£ Python trong kh·ªëi Markdown ````python ... ````:",
}
# --- K·∫æT TH√öC ƒêI·ªÄU CH·ªàNH ---

task = st.radio("Ch·ªçn t√°c v·ª•", list(TASK_PROMPTS.keys()), horizontal=True)
text = st.text_area("N·ªôi dung ƒë·∫ßu v√†o", height=180, placeholder="Nh·∫≠p ƒëo·∫°n vƒÉn ho·∫∑c y√™u c·∫ßu...", key="input_text")
st.divider()


# =============================
# 5Ô∏è‚É£ H√†m g·ªçi Ollama API
# (Gi·ªØ nguy√™n ph·∫ßn n√†y)
# =============================
def get_ollama_response(model, prompt, output_placeholder, task):
    full_response = ""
    st.info(f"ƒêang x·ª≠ l√Ω b·∫±ng m√¥ h√¨nh: **{model}** tr√™n Ollama...")
    error_msg = None

    try:
        response = client.chat.completions.create(
            model=model,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.7
        )
        full_response = response.choices[0].message.content

        if task == "T·∫°o m√£ Python":
            output_placeholder.code(full_response, language="python")
        else:
            output_placeholder.markdown(full_response)

        st.success("‚úÖ Ho√†n t·∫•t.")

    except Exception as e:
        if "Connection" in str(e) or "Failed to establish a new connection" in str(e):
            error_msg = f"‚ùå L·ªói k·∫øt n·ªëi: Ollama c√≥ ƒëang ch·∫°y kh√¥ng? Vui l√≤ng ki·ªÉm tra d·ªãch v·ª•!"
        else:
            error_msg = f"‚ùå ƒê√£ x·∫£y ra l·ªói: {e}"
        st.error(error_msg)

    return full_response, error_msg


# =============================
# 6Ô∏è‚É£, 7Ô∏è‚É£, 8Ô∏è‚É£ Ph·∫ßn c√≤n l·∫°i c·ªßa code (Gi·ªØ nguy√™n)
# =============================
def save_result(task, model, text, result):
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"result_{task.replace(' ', '_')}_{timestamp}_Ollama.txt"
    with open(filename, "w", encoding="utf-8") as f:
        f.write("=== LLM TOOL RESULT (Ollama Local) ===\n")
        f.write(f"Task: {task}\n")
        f.write(f"Model: {model}\n")
        f.write(f"Time: {timestamp}\n\n")
        f.write("----- Input -----\n")
        f.write(text.strip() + "\n\n")
        f.write("----- Output -----\n")
        f.write(result.strip() + "\n")
    return filename


if st.button("Ch·∫°y t√°c v·ª•"):
    if not text.strip():
        st.warning("Vui l√≤ng nh·∫≠p n·ªôi dung tr∆∞·ªõc khi x·ª≠ l√Ω.")
    else:
        current_model = OLLAMA_MODEL
        prompt = f"{TASK_PROMPTS[task]}\n\n{text}"
        st.subheader("üìù K·∫øt qu·∫£")
        output_box = st.empty()

        result, error_msg = get_ollama_response(current_model, prompt, output_box, task)

        if not error_msg:
            entry = {
                "time": datetime.now().strftime("%H:%M:%S"),
                "task": task,
                "model": current_model,
                "text": text,
                "result": result
            }
            st.session_state.history.insert(0, entry)

            filename = save_result(task, current_model, text, result)
            try:
                with open(filename, "r", encoding="utf-8") as file:
                    st.download_button(
                        label="‚¨áÔ∏è T·∫£i k·∫øt qu·∫£ (.txt)",
                        data=file.read(),
                        file_name=filename,
                        mime="text/plain",
                        use_container_width=True
                    )
            except Exception:
                st.warning("Kh√¥ng th·ªÉ t·∫°o file t·∫£i xu·ªëng.")

if st.session_state.history:
    st.divider()
    st.subheader("üìö L·ªãch s·ª≠ h·ªôi tho·∫°i")
    for i, item in enumerate(st.session_state.history):
        with st.expander(f"{item['time']} ‚Äì {item['task']} ({item['model']})"):
            st.markdown(f"**T√°c v·ª•:** {item['task']}")
            st.markdown(f"**M√¥ h√¨nh:** {item['model']}")
            st.markdown("**ƒê·∫ßu v√†o:**")
            st.code(item["text"], language="text")
            st.markdown("**K·∫øt qu·∫£:**")
            if item["task"] == "T·∫°o m√£ Python":
                st.code(item["result"], language="python")
            else:
                st.write(item["result"])
